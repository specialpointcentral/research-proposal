\section{Research Plan and Methodology}
\subsection{Design a tool to analyze legacy programs}
In order to complete the migration of a legacy system, the first step is
to analyze the program and identify the code and data that needs to be migrated.
There are three parts we need to consider: data analysis, recognition algorithm
and recognition accuracy.

The first is \textbf{data analysis}. The purpose of the analysis is to provide
information about binaries so that the subsequent identification process can
proceed smoothly.
There are many similar works in the field of binary rewriting to explore
the accuracy of binary analysis.
BIRD \cite{Nanda2006BIRDBI} uses a combination of static and dynamic identification
methods to improve the accuracy of the analysis. However, for dynamic disassembly,
some trapped instruction need to be inserted which is not a good choice for us.
Other papers point out that static disassembly can also achieve good results
\cite{Andriesse2016AnIA}. So we prepare to implement the static analysis tool first
and then decide whether to add dynamic analysis based on the accuracy of the analysis.

The next is the \textbf{recognition algorithm}.
Moat \cite{Sinha2015MoatVC} is a detection tool designed by Berkeley that uses
automatic theorem proving and information flow analysis methods to discover the
possibility of applications leakage of secret information in the SGX region by
analyzing the assembly language level of the program.
Our work can be based on Moat, from which we can extract effective identification
and verification algorithms and use them in our analysis tools. 

In order to evaluate \textbf{recognition accuracy}, we will consider it in two parts.
\textit{Coverage of analysis}:
We will use different test cases to see how well the overall analysis is covered.
Since we have access to the source code of these test programs,
the coverage of the analysis can be measured by some tools such as gcov \cite{GCOV}
and QEMU \cite{Bellard2005QEMUAF}.
\textit{Correctness of the analysis}:
We also use the test cases available in the source code to verify correctness.
We would like to compare the results of our analysis tool with the results of the source code
after automatic analysis by Glamdring \cite{Lind2017GlamdringAA} in order to obtain
an accurate analysis.

\subsection{Design a binary rewriting tool to protect confidential code and data}
With the help of analysis tools, it is easy to obtain the code segments that need
to be protected and the memory areas where important data is located.

\textbf{Code protection}. For code segments that need to be protected,
we can use the Minimal-invasive translation method. Similar to the rev.ng \cite{Federico2017revngAU}
and pin \cite{Luk2005PinBC}, we can insert the required functions before and
after the code segments. We insert the enclave's entry code and enclave's parameters passing code
before the segments, also enclave's return parameters can be built at the end of the segments.
There should be many more details to note and consider here that need to be
discovered and resolved in the course of research.

\textbf{Data protection}. Data protection is more difficult than code protection,
especially for global variables.
For local variables, we can analyze them, get the program boundary,
and put the variables as well as code into enclave for protection.
PtrSplit focuses on C/C++ pointers, identifies pointers that block
the generation of partition boundaries \cite{Liu2017PtrSplitSG}.
But for global variables, there is no good solution for now. However,
for a highly cohesive and low-coupling system, global variables are often not recommended
\cite{GlobalVariablesAreBad},
so dropping this part of the protection when it cannot be solved is generally not a big deal. 

\subsection{Extend the binary rewriting tool into distributed system}
Our work will explore two areas related to distributed systems, how to support legacy distributed
programs and how to enable tools to run on distributed systems.

\textbf{Support legacy distributed programs}.
These distributed legacy programs tend to have more complex features
compared to ordinary programs.
For example, OpenMP \cite{Dagum1998OpenMPAI} and MPI+OpenMP \cite{Klinkenberg2020CHAMELEONRL},
they will use mechanisms such as semaphores, message communication, etc., which cause problems
for both the identification and the transformation of our tools.
We will explore and tackle these challenges during our research process.

\textbf{Tool runs on distributed systems}.
How to run our tools on a distributed platform is a complex work.
DQEMU \cite{Zhao2020DQEMUAS} achieves a distributed dynamic binary translation system.
It discusses the implementation issues and performance optimization including
data coherence protocol, locking mechanism, system calls and remote thread migration.
We can take the idea of DQEMU and modify our tool to run on a distributed system.

\subsection{Optimize the above system by software-hardware co-optimization}
Whether running secret code in enclave or using rewriting tools for transformation,
they both introduce a significant performance overhead.
There are many studies investigated the overhead of trusted computing and 
the corresponding optimization method, including avoids enclave switches \cite{Tian2018SwitchlessCM}
and reduces page swap \cite{Orenbach2017EleosEO, Taassori2018VAULTRP}. But these optimization
maybe difficult to implement in our tool because it is difficult to change the
original behavior of the program. 
In addition, binary rewriting also faces the significant performance overhead, and existing
software optimizations are limited in dealing with these issues \cite{Kim2003HardwareSF}.

So our future work will summarize the existing optimization and search the performance
bottlenecks through performance analysis tools, such as Perf, VTune, etc.
After we obtain the bottlenecks of the performance, we can extract common features
and design/modify some hardware modules to speed up our tool and programs.